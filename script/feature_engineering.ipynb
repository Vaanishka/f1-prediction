{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "031011a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Checking c:\\Users\\trive\\Desktop\\vaanishka\\Data Science\\f1_pred\\data\\raw for Abu Dhabi data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Abu Dhabi Grand Prix - Practice 2 [v3.7.0]\n",
      "req            INFO \tNo cached data found for session_info. Loading data...\n",
      "_api           INFO \tFetching session info data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for driver_info. Loading data...\n",
      "_api           INFO \tFetching driver list...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for session_status_data. Loading data...\n",
      "_api           INFO \tFetching session status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for track_status_data. Loading data...\n",
      "_api           INFO \tFetching track status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for _extended_timing_data. Loading data...\n",
      "_api           INFO \tFetching timing data...\n",
      "_api           INFO \tParsing timing data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for timing_app_data. Loading data...\n",
      "_api           INFO \tFetching timing app data...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tNo cached data found for weather_data. Loading data...\n",
      "_api           INFO \tFetching weather data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for race_control_messages. Loading data...\n",
      "_api           INFO \tFetching race control messages...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '4', '5', '6', '10', '12', '14', '16', '18', '22', '23', '27', '30', '31', '43', '44', '55', '63', '81', '87']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: Abu Dhabi is now locked into the Master Cache.\n",
      "üìç All future race data will now be stored in: c:\\Users\\trive\\Desktop\\vaanishka\\Data Science\\f1_pred\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "import fastf1\n",
    "import os\n",
    "\n",
    "# 1. Point to your designated \"Master\" folder\n",
    "# Use the full path to be absolutely certain\n",
    "master_cache_dir = os.path.abspath('../data/raw/') \n",
    "\n",
    "if not os.path.exists(master_cache_dir):\n",
    "    os.makedirs(master_cache_dir)\n",
    "\n",
    "fastf1.Cache.enable_cache(master_cache_dir)\n",
    "\n",
    "# 2. THE VERIFICATION TEST\n",
    "# We attempt to load Abu Dhabi. \n",
    "# - If it's already there, it loads in 0.1 seconds from disk.\n",
    "# - If it's not there, FastF1 downloads it to THIS folder.\n",
    "try:\n",
    "    print(f\"üîÑ Checking {master_cache_dir} for Abu Dhabi data...\")\n",
    "    test_session = fastf1.get_session(2025, 'Abu Dhabi', 'FP2')\n",
    "    test_session.load(telemetry=False) \n",
    "    print(\"‚úÖ SUCCESS: Abu Dhabi is now locked into the Master Cache.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load session: {e}\")\n",
    "\n",
    "print(f\"üìç All future race data will now be stored in: {master_cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b09bb0",
   "metadata": {},
   "source": [
    "# Creating Processed Data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692cdc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Abu Dhabi Grand Prix - Practice 2 [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '4', '10', '11', '14', '16', '18', '20', '22', '23', '24', '27', '30', '43', '44', '55', '61', '63', '77', '81']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scrubbing Complete! 312 laps saved to ../data/processed/abu_dhabi_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fastf1\n",
    "\n",
    "# 1. Load the session (from your newly fixed cache)\n",
    "fastf1.Cache.enable_cache('../data/raw')\n",
    "session = fastf1.get_session(2024, 'Abu Dhabi', 'FP2')\n",
    "session.load(telemetry=False)\n",
    "\n",
    "# 2. THE SCRUB: Filter for high-quality data\n",
    "# We remove laps that aren't 'Accurate' and laps with pit stops\n",
    "clean_laps = session.laps.pick_accurate().copy()\n",
    "\n",
    "# 3. Add a 'Fuel-Corrected' column (using your rules.json)\n",
    "# (Assuming you've loaded your rules.json as 'rules' earlier in the notebook)\n",
    "# For now, let's just mark them for processing\n",
    "clean_laps['Is_Model_Ready'] = True\n",
    "\n",
    "# 4. SAVE to Processed folder\n",
    "# This is where your model will eventually 'eat' from\n",
    "output_path = '../data/processed/abu_dhabi_clean.csv'\n",
    "clean_laps.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Scrubbing Complete! {len(clean_laps)} laps saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dbef2",
   "metadata": {},
   "source": [
    "# FUEL CORRECTION\n",
    "**The Logic: \"Subtracting the Weight\"**\n",
    "We know the car starts the race with roughly 100kg of fuel and ends with almost 0kg.\n",
    "\n",
    "- The code will calculate which lap number we are on.\n",
    "- It will estimate the fuel remaining.\n",
    "- It will multiply that weight by your fuel_weight_penalty_per_10kg (0.3s).\n",
    "- It will subtract that \"penalty\" from the actual lap time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49212632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fuel Normalization Complete.\n",
      "Sample: Lap 1 actual time was 0 days 00:01:25.980000, Naked time is 84.638s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Load your Rules and your Cleaned Data\n",
    "with open('../coefficient/rules.json', 'r') as f:\n",
    "    rules = json.load(f)\n",
    "\n",
    "df = pd.read_csv('../data/processed/abu_dhabi_clean.csv')\n",
    "\n",
    "# 2. Extract the constant from your JSON\n",
    "penalty_per_10kg = rules['physics_constants']['fuel_weight_penalty_per_10kg']\n",
    "fuel_burn_per_lap = rules['physics_constants']['average_fuel_burn_per_lap_kg']\n",
    "\n",
    "# 3. Calculate Fuel-Corrected Time\n",
    "# We assume a full tank (100kg) at Lap 1\n",
    "def calculate_fuel_correction(row):\n",
    "    laps_remaining = df['LapNumber'].max() - row['LapNumber']\n",
    "    fuel_on_board = laps_remaining * fuel_burn_per_lap\n",
    "    \n",
    "    # Correction in seconds: (Weight / 10) * Penalty\n",
    "    correction_seconds = (fuel_on_board / 10) * penalty_per_10kg\n",
    "    \n",
    "    # Subtract the penalty from the actual lap time\n",
    "    # (Note: We convert LapTime to total seconds first)\n",
    "    actual_seconds = pd.to_timedelta(row['LapTime']).total_seconds()\n",
    "    return actual_seconds - correction_seconds\n",
    "\n",
    "df['Naked_LapTime_Seconds'] = df.apply(calculate_fuel_correction, axis=1)\n",
    "\n",
    "# 4. Save this 'Feature Engineered' data\n",
    "df.to_csv('../data/processed/abu_dhabi_clean.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Fuel Normalization Complete.\")\n",
    "print(f\"Sample: Lap 1 actual time was {df.iloc[0]['LapTime']}, Naked time is {df.iloc[0]['Naked_LapTime_Seconds']:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01dc3c",
   "metadata": {},
   "source": [
    "# Tire normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae40de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tire Normalization Fixed!\n",
      "Using your slopes: Soft=0.08s, Medium=0.05s, Hard=0.03s\n"
     ]
    }
   ],
   "source": [
    "# 1. Map your specific JSON keys to the Tire Compounds\n",
    "# This connects 'SOFT' from the telemetry to 'soft_slope' from your JSON\n",
    "deg_rules = rules.get('degradation_coefficients', {})\n",
    "\n",
    "compound_map = {\n",
    "    'SOFT': deg_rules.get('soft_slope', 0.08),\n",
    "    'MEDIUM': deg_rules.get('medium_slope', 0.05),\n",
    "    'HARD': deg_rules.get('hard_slope', 0.03)\n",
    "}\n",
    "\n",
    "def calculate_tire_correction(row):\n",
    "    # Get the compound (e.g., 'SOFT') and the age (e.g., 5 laps)\n",
    "    compound = str(row['Compound']).upper()\n",
    "    age = row['TyreLife']\n",
    "    \n",
    "    # Get the slope from our map; if it's a 'TEST' tire, default to Medium (0.05)\n",
    "    slope = compound_map.get(compound, 0.05)\n",
    "    \n",
    "    wear_tax = age * slope\n",
    "    return row['Naked_LapTime_Seconds'] - wear_tax\n",
    "\n",
    "# Apply the correction\n",
    "df['Ultimate_Baseline_Seconds'] = df.apply(calculate_tire_correction, axis=1)\n",
    "\n",
    "# 2. Save the final processed data\n",
    "df.to_csv('../data/processed/abu_dhabi_clean.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Tire Normalization Fixed!\")\n",
    "print(f\"Using your slopes: Soft={compound_map['SOFT']}s, Medium={compound_map['MEDIUM']}s, Hard={compound_map['HARD']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db443a9d",
   "metadata": {},
   "source": [
    "# foundation : Rolling Best, Overdrive, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35ec081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Master Baseline successfully generated for: Abu Dhabi Grand Prix\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. FIXED PATH: Pointing to the coefficients folder\n",
    "# Adjust the '../' depending on your current working directory\n",
    "json_path = '../coefficient/track_dna.json'\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'r') as f:\n",
    "        track_dna = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Could not find track_dna.json at {json_path}\")\n",
    "    # Optional: stop execution if the file is missing\n",
    "    raise\n",
    "\n",
    "# 2. Load the cleaned session data\n",
    "df = pd.read_csv('../data/processed/abu_dhabi_clean.csv')\n",
    "\n",
    "# 3. Pull DNA based on the 'EventName' column\n",
    "# We use .get() with a default to prevent the script from crashing\n",
    "event_key = df['EventName'].iloc[0] if 'EventName' in df.columns else \"Abu Dhabi Grand Prix\"\n",
    "dna = track_dna.get(event_key, track_dna.get(\"Abu Dhabi Grand Prix\"))\n",
    "\n",
    "# --- FEATURE ENGINEERING ENGINE ---\n",
    "tire_slopes = {'SOFT': 0.082, 'MEDIUM': 0.045, 'HARD': 0.021}\n",
    "\n",
    "def engineer_lap_features(row):\n",
    "    try:\n",
    "        # Handle potential time format errors\n",
    "        actual_time = pd.to_timedelta(row['LapTime']).total_seconds()\n",
    "        \n",
    "        # Ensure LapNumber is a number\n",
    "        lap_num = float(row.get('LapNumber', 1))\n",
    "        \n",
    "        # A. Fuel Normalization\n",
    "        current_fuel = max(0, 100 - (lap_num * dna['fuel_burn_lap']))\n",
    "        fuel_correction = current_fuel * dna['penalty_per_kg']\n",
    "        \n",
    "        # B. Tire Degradation\n",
    "        compound = str(row.get('Compound', 'SOFT')).upper()\n",
    "        tire_correction = float(row.get('TyreLife', 1)) * tire_slopes.get(compound, 0.045)\n",
    "        \n",
    "        # C. Mechanical & Environment\n",
    "        drs_boost = 0.85 if row.get('DRS_Active') == 1 else 0\n",
    "        weather_tax = max(0, (float(row.get('AirTemp', 28)) - 20) * 0.015)\n",
    "\n",
    "        # Naked Pace Calculation\n",
    "        naked_pace = actual_time - fuel_correction - tire_correction + drs_boost - weather_tax\n",
    "        \n",
    "        return pd.Series([naked_pace, fuel_correction, tire_correction, drs_boost])\n",
    "    except Exception as e:\n",
    "        # This catches specific row errors without stopping the whole script\n",
    "        return pd.Series([np.nan] * 4)\n",
    "\n",
    "# Apply the Physics Layer\n",
    "df[['Naked_Pace', 'Fuel_Tax', 'Tire_Tax', 'DRS_Correction']] = df.apply(engineer_lap_features, axis=1)\n",
    "df = df.dropna(subset=['Naked_Pace'])\n",
    "\n",
    "# --- BEHAVIORAL LAYER ---\n",
    "# Peak Potential: The true ceiling (Best in 3-lap window)\n",
    "df['Peak_Potential'] = df.groupby('Driver')['Naked_Pace'].transform(lambda x: x.rolling(window=3, min_periods=1).min())\n",
    "\n",
    "# Sandbagging Delta: Identifying hidden pace\n",
    "df['Sandbag_Delta'] = df['Naked_Pace'] - df['Peak_Potential']\n",
    "\n",
    "# Track Evolution: FIXED logic to use row-wise lap number safely\n",
    "df['Final_Baseline'] = df['Naked_Pace'] - (df['LapNumber'].astype(float) * 0.008)\n",
    "\n",
    "# 4. Save the Model-Ready dataset\n",
    "df.to_csv('../data/processed/master_baseline.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Master Baseline successfully generated for: {event_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c1830",
   "metadata": {},
   "source": [
    "# Sector Analysis Script \n",
    "**Why this matters for 2026 Model**\n",
    "- **Chassis Score:** If a team (like McLaren) has a low S3_Chassis_Score, they will benefit more from the 2026 weight reduction.\n",
    "- **Clipping Risk:** Teams with high top-end power today will face the biggest challenge in 2026 when the electrical boost runs out at $340\\text{km/h}$.\n",
    "**Sandbag-Proofing:** Drivers can't easily sandbag in Sector 3 without looking \"slow\" in the data. If their S3 is elite but their S1 is poor, we know they have their engine turned down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425222dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Type Error Squashed! Sector Analysis Complete.\n",
      "  Driver Compound  Sector1Time  Sector2Time  Sector3Time  Sandbag_Delta  \\\n",
      "0    ALB   MEDIUM       17.514       36.583       31.210      19.506133   \n",
      "1    ALB     SOFT       17.223       36.299       30.747      11.696211   \n",
      "2    ALO   MEDIUM       17.574       36.758       31.137       6.379250   \n",
      "3    ALO     SOFT       17.264       36.262       30.912      18.582314   \n",
      "4    BOT   MEDIUM       17.610       36.651       31.087      12.499056   \n",
      "\n",
      "   S3_Chassis_Score  S1_Power_Score  2026_Clipping_Risk  \n",
      "0          1.026746        1.028360           21.087688  \n",
      "1          1.011514        1.011274           12.839343  \n",
      "2          1.024345        1.031883            7.614523  \n",
      "3          1.016942        1.013681           19.850219  \n",
      "4          1.022700        1.033997           13.957981  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the Master Baseline\n",
    "df = pd.read_csv('../data/processed/master_baseline.csv')\n",
    "\n",
    "# --- FIX: Convert Sector Times to Numeric Seconds ---\n",
    "# This ensures we aren't trying to divide \"str\" by \"str\"\n",
    "sector_cols = ['Sector1Time', 'Sector2Time', 'Sector3Time']\n",
    "\n",
    "for col in sector_cols:\n",
    "    # Convert to timedelta then to total seconds\n",
    "    df[col] = pd.to_timedelta(df[col]).dt.total_seconds()\n",
    "\n",
    "# 2. Pivot the data to see Sector Performance\n",
    "sector_perf = df.groupby(['Driver', 'Compound']).agg({\n",
    "    'Sector1Time': 'min',\n",
    "    'Sector2Time': 'min',\n",
    "    'Sector3Time': 'min',\n",
    "    'Sandbag_Delta': 'mean' \n",
    "}).reset_index()\n",
    "\n",
    "# 3. Calculate \"Sector Dominance\" (Safe from TypeErrors now)\n",
    "# We divide by the fastest time in that sector to get a ratio (1.0 = fastest)\n",
    "sector_perf['S3_Chassis_Score'] = sector_perf['Sector3Time'] / sector_perf['Sector3Time'].min()\n",
    "sector_perf['S1_Power_Score'] = sector_perf['Sector1Time'] / sector_perf['Sector1Time'].min()\n",
    "\n",
    "# 4. 2026 Prediction Feature: \"The Clipping Risk\"\n",
    "sector_perf['2026_Clipping_Risk'] = sector_perf['S1_Power_Score'] * (1 + sector_perf['Sandbag_Delta'])\n",
    "\n",
    "# 5. Save the Sector DNA\n",
    "sector_perf.to_csv('../data/processed/sector_analysis.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Type Error Squashed! Sector Analysis Complete.\")\n",
    "print(sector_perf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5224dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ All Advanced Features Engineered!\n",
      "  Driver  Naked_Pace  Overdrive_Score  Terrain_Edge  Grid_Position  \\\n",
      "0    ALB     82.0960        19.169117     -0.023870          148.0   \n",
      "1    ALO     82.3449        10.855169     -0.084608          201.0   \n",
      "2    BOT     81.9522        18.353978     -0.028193           45.0   \n",
      "3    COL     82.7616        24.477945      0.143069          372.0   \n",
      "4    DOO     82.8441        20.358792     -0.077092          380.0   \n",
      "\n",
      "   Sandbag_Delta  \n",
      "0        46.2961  \n",
      "1        44.5962  \n",
      "2        55.8171  \n",
      "3        60.1661  \n",
      "4        59.9441  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load our previous master file\n",
    "df = pd.read_csv('../data/processed/master_baseline.csv')\n",
    "\n",
    "# Ensure Sector times are numeric (Fixes your previous error)\n",
    "for col in ['Sector1Time', 'Sector2Time', 'Sector3Time']:\n",
    "    df[col] = pd.to_timedelta(df[col]).dt.total_seconds()\n",
    "\n",
    "# --- 1. OVERDRIVE SCORE (œÉ) ---\n",
    "# We calculate consistency per driver, per stint\n",
    "df['Overdrive_Score'] = df.groupby(['Driver', 'Stint'])['Naked_Pace'].transform('std')\n",
    "\n",
    "# --- 2. TERRAIN EDGE INDEX ---\n",
    "# Identify which drivers are gaining time in Technical (S3) vs Power (S1) sectors\n",
    "# Lower score = faster relative to the field\n",
    "df['S1_Ratio'] = df['Sector1Time'] / df['Sector1Time'].min()\n",
    "df['S3_Ratio'] = df['Sector3Time'] / df['Sector3Time'].min()\n",
    "df['Terrain_Edge'] = df['S3_Ratio'] - df['S1_Ratio'] \n",
    "# Positive Edge = Stronger in corners (Good for 2026 weight reduction)\n",
    "# Negative Edge = Stronger on straights (Risky for 2026 energy clipping)\n",
    "\n",
    "# --- 3. GRID POSITION INTEGRATION ---\n",
    "# Usually, we'd join this from a separate 'Qualifying' session file\n",
    "# For now, let's create a placeholder that simulates the Grid Rank\n",
    "# (In your real pipeline, you would merge df with session.results[['Driver', 'GridPosition']])\n",
    "df['Grid_Position'] = df.groupby('Driver')['Naked_Pace'].transform('min').rank(method='min')\n",
    "\n",
    "# --- FINAL AGGREGATION FOR ML ---\n",
    "# We want one row per driver that sums up their '2024 Profile'\n",
    "model_ready = df.groupby('Driver').agg({\n",
    "    'Naked_Pace': 'min',        # Their absolute peak\n",
    "    'Overdrive_Score': 'mean',  # Their average consistency\n",
    "    'Terrain_Edge': 'mean',     # Their car's DNA\n",
    "    'Grid_Position': 'first',   # Where they start\n",
    "    'Sandbag_Delta': 'max'      # How much they were hiding\n",
    "}).reset_index()\n",
    "\n",
    "model_ready.to_csv('../data/processed/model_training_set.csv', index=False)\n",
    "\n",
    "print(\"üèÜ All Advanced Features Engineered!\")\n",
    "print(model_ready.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbd516",
   "metadata": {},
   "source": [
    "**Pulling race data from the api to get data for dirty air an dslip stream added the calculated coefficients to data/processed/multi_race_traffic.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6f7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req         WARNING \tDEFAULT CACHE ENABLED! (96.75 MB) C:\\Users\\trive\\AppData\\Local\\Temp\\fastf1\n",
      "core           INFO \tLoading data for Italian Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tNo cached data found for session_info. Loading data...\n",
      "_api           INFO \tFetching session info data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for driver_info. Loading data...\n",
      "_api           INFO \tFetching driver list...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for session_status_data. Loading data...\n",
      "_api           INFO \tFetching session status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for lap_count. Loading data...\n",
      "_api           INFO \tFetching lap count data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for track_status_data. Loading data...\n",
      "_api           INFO \tFetching track status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for _extended_timing_data. Loading data...\n",
      "_api           INFO \tFetching timing data...\n",
      "_api           INFO \tParsing timing data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for timing_app_data. Loading data...\n",
      "_api           INFO \tFetching timing app data...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tNo cached data found for race_control_messages. Loading data...\n",
      "_api           INFO \tFetching race control messages...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '4', '81', '16', '63', '44', '23', '5', '12', '6', '55', '87', '22', '30', '31', '10', '43', '18', '14', '27']\n",
      "core           INFO \tLoading data for Hungarian Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tNo cached data found for session_info. Loading data...\n",
      "_api           INFO \tFetching session info data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for driver_info. Loading data...\n",
      "_api           INFO \tFetching driver list...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for session_status_data. Loading data...\n",
      "_api           INFO \tFetching session status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for lap_count. Loading data...\n",
      "_api           INFO \tFetching lap count data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for track_status_data. Loading data...\n",
      "_api           INFO \tFetching track status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for _extended_timing_data. Loading data...\n",
      "_api           INFO \tFetching timing data...\n",
      "_api           INFO \tParsing timing data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for timing_app_data. Loading data...\n",
      "_api           INFO \tFetching timing app data...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tNo cached data found for race_control_messages. Loading data...\n",
      "_api           INFO \tFetching race control messages...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tFinished loading data for 20 drivers: ['4', '81', '63', '16', '14', '5', '18', '30', '1', '12', '6', '44', '27', '55', '23', '31', '22', '43', '10', '87']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ multi_race_traffic.csv is now fixed and saved with Slipstream_Delta.\n"
     ]
    }
   ],
   "source": [
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Monza and Hungary without heavy telemetry\n",
    "traffic_races = [(2025, 'Monza', 'R'), (2025, 'Hungary', 'R')]\n",
    "traffic_list = []\n",
    "\n",
    "for yr, loc, sess_type in traffic_races:\n",
    "    s = fastf1.get_session(yr, loc, sess_type)\n",
    "    s.load(telemetry=False, weather=False) # Lightweight load\n",
    "    laps = s.laps.copy()\n",
    "    # Calculate Gap for traffic detection logic\n",
    "    laps['GapToPrev'] = laps.sort_values(by=['LapNumber', 'Time']).groupby('LapNumber')['Time'].diff().dt.total_seconds()\n",
    "    traffic_list.append(laps)\n",
    "\n",
    "df_traffic = pd.concat(traffic_list).reset_index(drop=True)\n",
    "df_traffic['Is_Following'] = np.where(df_traffic['GapToPrev'] < 1.2, 1, 0)\n",
    "\n",
    "# 2. Calculate the \"Missing\" columns for the joiner\n",
    "# Map clean air max speed to every driver row\n",
    "clean_speeds = df_traffic[df_traffic['Is_Following'] == 0].groupby('Driver')['SpeedST'].max().rename('CleanMax')\n",
    "df_traffic = df_traffic.merge(clean_speeds, on='Driver', how='left')\n",
    "\n",
    "# The column your joiner is missing:\n",
    "df_traffic['Slipstream_Delta'] = np.where(df_traffic['Is_Following'] == 1, df_traffic['SpeedST'] - df_traffic['CleanMax'], 0)\n",
    "df_traffic['Dirty_Air_Tax'] = df_traffic.groupby('Driver')['LapTime'].transform(lambda x: x.dt.total_seconds() - x.dt.total_seconds().min())\n",
    "\n",
    "# 3. Save the fixed file to disk\n",
    "df_traffic.to_csv('../data/processed/multi_race_traffic.csv', index=False)\n",
    "print(\"‚úÖ multi_race_traffic.csv is now fixed and saved with Slipstream_Delta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689fd72",
   "metadata": {},
   "source": [
    "**merging df to point to the right location **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41841f",
   "metadata": {},
   "source": [
    "**checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15af08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sector data verified and updated for 2026 training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df_sector = pd.read_csv('../data/processed/sector_analysis.csv')\n",
    "\n",
    "# Use actual performance metrics as the 2026 foundation\n",
    "# We are mapping Sector Times to the features the XGBoost model expects\n",
    "df_sector['power_score'] = df_sector['Sector3Time']   # Real speed data\n",
    "df_sector['chassis_score'] = df_sector['Sector2Time'] # Real handling data\n",
    "df_sector['clipping_risk'] = df_sector['Sector1Time'].pct_change().fillna(0)\n",
    "\n",
    "# Save the accurate foundation\n",
    "df_sector.to_csv('../data/processed/sector_analysis.csv', index=False)\n",
    "print(\"‚úÖ Sector data verified and updated for 2026 training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2ba74",
   "metadata": {},
   "source": [
    "# SESSION JOIN\n",
    "\n",
    "This code is the \"Session Join\" because it unifies separate physics, traffic, and tactical files into a single master table, allowing the XGBoost model to see how variables like Grid Position and Edge Index interact to determine the final race result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41032d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ MASTER DATABASE LOCKED.\n",
      "Verified Columns: ['Driver', 'Naked_Pace_x', 'Overdrive_Score', 'Terrain_Edge', 'Grid_Position', 'Sandbag_Delta_x', 'Time', 'DriverNumber', 'LapTime', 'LapNumber', 'Stint', 'PitOutTime', 'PitInTime', 'Sector1Time', 'Sector2Time', 'Sector3Time', 'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'IsPersonalBest', 'Compound', 'TyreLife', 'FreshTyre', 'Team', 'LapStartTime', 'LapStartDate', 'TrackStatus', 'Position', 'Deleted', 'DeletedReason', 'FastF1Generated', 'IsAccurate', 'DRS_Active', 'Naked_Pace_y', 'Fuel_Tax', 'Tire_Tax', 'DRS_Correction', 'Peak_Potential', 'Sandbag_Delta_y', 'Final_Baseline', 'Dirty_Air_Tax', 'Slipstream_Delta', 'chassis_score', 'power_score', 'clipping_risk']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the \"Truth\" data (Meta/Targets)\n",
    "df_meta = pd.read_csv('../data/processed/model_training_set.csv')\n",
    "\n",
    "# 2. Load the \"Performance\" data (Features)\n",
    "df_baseline = pd.read_csv('../data/processed/master_baseline.csv')\n",
    "df_traffic = pd.read_csv('../data/processed/multi_race_traffic.csv')\n",
    "df_sector = pd.read_csv('../data/processed/sector_analysis.csv')\n",
    "\n",
    "# 3. Aggregations (Ensuring we use your specific S3/S1 names)\n",
    "traffic_profile = df_traffic.groupby('Driver').agg({\n",
    "    'Dirty_Air_Tax': 'mean',\n",
    "    'Slipstream_Delta': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort by LapTime to get the peak potential for each driver\n",
    "df_sector = df_sector.sort_values(by=['Driver', 'Sector1Time'], ascending=True)\n",
    "sector_profile = df_sector.groupby('Driver').agg({\n",
    "    'S3_Chassis_Score': 'first',      \n",
    "    'S1_Power_Score': 'first',        \n",
    "    '2026_Clipping_Risk': 'mean'      \n",
    "}).reset_index()\n",
    "sector_profile.columns = ['Driver', 'chassis_score', 'power_score', 'clipping_risk']\n",
    "\n",
    "# 4. THE GRAND MERGE (Including the Grid/Terrain/Overdrive data)\n",
    "# First, merge the features together\n",
    "features = pd.merge(df_baseline, traffic_profile, on='Driver', how='inner')\n",
    "features = pd.merge(features, sector_profile, on='Driver', how='inner')\n",
    "\n",
    "# SECOND, join with the Meta/Target file\n",
    "# This attaches: GridPosition, Terrain, Overdrive_Naked_Score, and Sandbag_Delta\n",
    "final_df = pd.merge(df_meta, features, on='Driver', how='inner')\n",
    "\n",
    "# 5. Lock it down\n",
    "final_df.to_csv('../data/final_db/final_training_data_2026.csv', index=False)\n",
    "\n",
    "print(\"üèÅ MASTER DATABASE LOCKED.\")\n",
    "print(f\"Verified Columns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf8f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
